{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "# Download PDF file\n",
    "import os\n",
    "import requests\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm \n",
    "\n",
    "def iterator(obj, istqdm = False):\n",
    "    if istqdm:\n",
    "        return tqdm(obj)\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "# Get PDF document\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "  # The URL of the PDF you want to download\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  # The local filename to save the downloaded file\n",
    "  filename = pdf_path\n",
    "\n",
    "  # Send a GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      # Open a file in binary write mode and save the content to it\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
    "\n",
    "    # Other potential text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str, istqdm = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in iterator(enumerate(doc), istqdm):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize NLP pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x154928006d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spacy_nlp(pages_and_texts: dict, istqdm = False) -> dict:\n",
    "    for item in iterator(pages_and_texts, istqdm):\n",
    "        item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "        \n",
    "        # Make sure all sentences are strings\n",
    "        item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "        \n",
    "        # Count the sentences \n",
    "        item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "def chunk_sentences(pages_and_texts: dict, num_sentence_chunk_size: int, istqdm = False) -> dict:\n",
    "    # Loop through pages and texts and split sentences into chunks+\n",
    "    for item in iterator(pages_and_texts, istqdm):\n",
    "        item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                            slice_size=num_sentence_chunk_size)\n",
    "        item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "    return pages_and_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning metadata to each chunk and restructuring the data structure into a list of chunks with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Split each chunk into its own item\n",
    "# This is used at the very end to use the indicies to reference the pages\n",
    "\n",
    "def restructure_chunks(pages_and_texts: dict, istqdm = False) -> dict:\n",
    "    pages_and_chunks = []\n",
    "    \n",
    "    for item in iterator(pages_and_texts, istqdm):\n",
    "        for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "            \n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "    return pages_and_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out by the preset min_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it feels like this could be done more intelligently, but it's a good starting point\n",
    "import pandas as pd\n",
    "\n",
    "def filter_pages_and_texts(pages_and_chunks: dict, \n",
    "                           min_token_length: int) -> dict:\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
    "    return pages_and_chunks_over_min_token_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and test the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model_name = \"all-mpnet-base-v2\"\n",
    "embedding_model = SentenceTransformer(model_name_or_path=embedding_model_name, \n",
    "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "#embedding_model.to(\"cuda\")\n",
    "def get_embedding(text):\n",
    "    url = \"http://localhost:49152/api/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['embedding']\n",
    "\n",
    "def apply_ollama_embeddings(pages_and_chunks_over_min_token_len: dict, \n",
    "                     istqdm = False) -> dict:\n",
    "    \n",
    "    for item in iterator(pages_and_chunks_over_min_token_len, istqdm):\n",
    "        item[\"embedding\"] = get_embedding(item['sentence_chunk'])\n",
    "    \n",
    "    return pages_and_chunks_over_min_token_len\n",
    "\n",
    "def apply_embeddings(pages_and_chunks_over_min_token_len: dict, \n",
    "                     embedding_model: SentenceTransformer,\n",
    "                     istqdm = False,\n",
    "                     flatten = False) -> dict:\n",
    "    if not flatten:\n",
    "        for item in iterator(pages_and_chunks_over_min_token_len, istqdm):\n",
    "            item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"],\n",
    "                                                batch_size=32,\n",
    "                                                convert_to_tensor=True)\n",
    "    else:\n",
    "        text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
    "        embeddings = embedding_model.encode(text_chunks)\n",
    "        print(embeddings)\n",
    "        for embedding, item in iterator(zip(embeddings, pages_and_chunks_over_min_token_len), istqdm):\n",
    "            item[\"embedding\"] = embedding\n",
    "    \n",
    "    return pages_and_chunks_over_min_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06742426  0.09022821 -0.00509549 ... -0.02211546 -0.02321364\n",
      "   0.01256908]\n",
      " [ 0.0552156   0.0592139  -0.01661676 ... -0.01204061 -0.01028473\n",
      "   0.02273969]\n",
      " [ 0.02798019  0.03398141 -0.02064265 ... -0.00536189  0.02125598\n",
      "   0.03130551]\n",
      " ...\n",
      " [ 0.07705151  0.00978554 -0.01218174 ... -0.04086804 -0.07517631\n",
      "  -0.02405259]\n",
      " [ 0.10304512 -0.0164702   0.00826845 ... -0.05742176 -0.02828029\n",
      "  -0.02946858]\n",
      " [ 0.08637737 -0.01253593 -0.01127469 ... -0.05223795 -0.03367297\n",
      "  -0.02986604]]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts = apply_spacy_nlp(pages_and_texts)\n",
    "pages_and_texts = chunk_sentences(pages_and_texts,num_sentence_chunk_size = 10 )\n",
    "pages_and_chunks = restructure_chunks(pages_and_texts)\n",
    "pages_and_chunks = filter_pages_and_texts(pages_and_chunks,30)\n",
    "embedded_pages_and_chunks = apply_embeddings(pages_and_chunks,embedding_model,flatten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=49151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(name='testing_python_creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_or_create_collection(name='testing_python_creation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"all-mpnet-base-v2\"\n",
    "path = 'C:\\\\Users\\\\crossfire234\\\\Desktop\\\\Software\\\\LLMs\\\\simple-local-rag\\\\simple-local-rag\\\\human-nutrition-text.pdf'\n",
    "file_name = os.path.basename(path)\n",
    "collection.add(\n",
    "    documents=[item['sentence_chunk'] for item in embedded_pages_and_chunks],\n",
    "    metadatas=[{\n",
    "        'page_number': item['page_number'],\n",
    "        'char_count': item['chunk_char_count'],\n",
    "        'word_count': item['chunk_word_count'],\n",
    "        'token_count': item['chunk_token_count'],\n",
    "        'embedding_model': embedding_model_name,\n",
    "        'url': path\n",
    "    } for item in embedded_pages_and_chunks],\n",
    "    ids=[f\"{file_name}_chunk_{i}\" for i in range(len(embedded_pages_and_chunks))],\n",
    "    embeddings=[item['embedding'].tolist() for item in embedded_pages_and_chunks]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"macronutrients functions\"\n",
    "query_embedding = embedding_model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macronutrients Nutrients that are needed in large amounts are called macronutrients. There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions. A unit of measurement of food energy is the calorie. On nutrition food labels the amount given for “calories” is actually equivalent to each calorie multiplied by one thousand. A kilocalorie (one thousand calories, denoted with a small “c”) is synonymous with the “Calorie” (with a capital “C”) on nutrition food labels. Water is also a macronutrient in the sense that you require a large amount of it, but unlike the other macronutrients, it does not yield calories. Carbohydrates Carbohydrates are molecules composed of carbon, hydrogen, and oxygen.\n",
      "Water There is one other nutrient that we must have in large quantities: water. Water does not contain carbon, but is composed of two hydrogens and one oxygen per molecule of water. More than 60 percent of your total body weight is water. Without it, nothing could be transported in or out of the body, chemical reactions would not occur, organs would not be cushioned, and body temperature would fluctuate widely. On average, an adult consumes just over two liters of water per day from food and drink combined. Since water is so critical for life’s basic processes, the amount of water input and output is supremely important, a topic we will explore in detail in Chapter 4. Micronutrients Micronutrients are nutrients required by the body in lesser amounts, but are still essential for carrying out bodily functions. Micronutrients include all the essential minerals and vitamins. There are sixteen essential minerals and thirteen vitamins (See Table 1.1 “Minerals and Their Major Functions” and Table 1.2 “Vitamins and Their Major Functions” for a complete list and their major functions). In contrast to carbohydrates, lipids, and proteins, micronutrients are not sources of energy (calories), but they assist in the process as cofactors or components of enzymes (i.e., coenzymes).\n",
      "Learning Objectives By the end of this chapter, you will be able to: • Describe basic concepts in nutrition • Describe factors that affect your nutritional needs • Describe the importance of research and scientific methods to understanding nutrition What are Nutrients? The foods we eat contain nutrients. Nutrients are substances required by the body to perform its basic functions. Nutrients must be obtained from our diet, since the human body does not synthesize or produce them. Nutrients have one or more of three basic functions: they provide energy, contribute to body structure, and/or regulate chemical processes in the body. These basic functions allow us to detect and respond to environmental surroundings, move, excrete wastes, respire (breathe), grow, and reproduce. There are six classes of nutrients required for the body to function and maintain overall health. These are carbohydrates, lipids, proteins, water, vitamins, and minerals. Foods also contain non-nutrients that may be harmful (such as natural toxins common in plant foods and additives like some dyes and preservatives) or beneficial (such as antioxidants). 4 | Introduction\n",
      "Vitamins Major Functions Water-soluble Thiamin (B1) Coenzyme, energy metabolism assistance Riboflavin (B2 ) Coenzyme, energy metabolism assistance Niacin (B3) Coenzyme, energy metabolism assistance Pantothenic acid (B5) Coenzyme, energy metabolism assistance Pyridoxine (B6) Coenzyme, amino acid synthesis assistance Biotin (B7) Coenzyme, amino acid and fatty acid metabolism Folate (B9) Coenzyme, essential for growth Cobalamin (B12) Coenzyme, red blood cell synthesis C (ascorbic acid) Collagen synthesis, antioxidant Fat-soluble A Vision, reproduction, immune system function D Bone and teeth health maintenance, immune system function E Antioxidant, cell membrane protection K Bone and teeth health maintenance, blood clotting Vitamin deficiencies can cause severe health problems and even death. For example, a deficiency in niacin causes a disease called pellagra, which was common in the early twentieth century in some parts of America. The common signs and symptoms of pellagra are known as the “4D’s—diarrhea, dermatitis, dementia, and death.” Until scientists found out that better diets relieved the signs and symptoms of pellagra, many people with the disease ended up hospitalized in insane asylums awaiting death. Other vitamins were also found to prevent certain disorders and diseases such as scurvy (vitamin C), night blindness vitamin A, and rickets (vitamin D). Table 1.3 Functions of Nutrients Introduction | 11\n",
      "Figure 1.1 The Macronutrie nts: Carbohydrat es, Lipids, Protein, and Water Proteins Proteins are macromolecules composed of chains of subunits called amino acids. Amino acids are simple subunits composed of carbon, oxygen, hydrogen, and nitrogen. Food sources of proteins include meats, dairy products, seafood, and a variety of different plant- based foods, most notably soy. The word protein comes from a Greek word meaning “of primary importance,” which is an apt description of these macronutrients; they are also known colloquially as the “workhorses” of life. Proteins provide four kilocalories of energy per gram; however providing energy is not protein’s most important function. Proteins provide structure to bones, muscles and skin, and play a role in conducting most of the chemical reactions that take place in the body. Scientists estimate that greater than one-hundred thousand different proteins exist within the human body. The genetic codes in DNA are basically protein recipes that determine the order in which 20 different amino acids are bound together to make thousands of specific proteins. Figure 1.1 The Macronutrients: Carbohydrates, Lipids, Protein, and Water Introduction | 7\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=5\n",
    ")\n",
    "for result in results['documents'][0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking local GPU memory availability\n",
    "\n",
    "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
    "\n",
    "> **Note:** You can also check this with the `!nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 11 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick gemma model based on GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 11 | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\n",
      "use_quantization_config set to: False\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    }
   ],
   "source": [
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticating hugging face account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\crossfire234\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# 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\n",
    "token = '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'\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama API\n",
    "- how do I get the tokenizer\n",
    "- what configuration is available\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "- gpu config\n",
    "docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "- how do I write this as a docker compose\n",
    "- what was the way to persist storage on chroma update?\n",
    "https://www.youtube.com/watch?v=61kaK-e3Owc&t=634s (Seems to just require the host volume to be mounted)\n",
    "```volumes:\n",
    "      - ./chroma_data:/chroma/chroma\n",
    "```\n",
    "\n",
    "#### Raw mode\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#request-raw-mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34813da9676443e8588b1cff15c5ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
    "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
    "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention \n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory \n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU \n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print number of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5012344832, 'model_mem_mb': 4780.14, 'model_mem_gb': 4.67}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the LLM with the test prompt to get the embedding output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test questions (queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to apply a base prompt to every chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join(context_items)\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the LLM with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query,\n",
    "        embedding_model,\n",
    "        collection,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=5\n",
    ")\n",
    "    context_items = [result for result in results['documents'][0]]\n",
    "\n",
    "    # Add score to context item\n",
    "    \"\"\" for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU  \"\"\"\n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the macronutrients, and what roles do they play in the human body?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\crossfire234\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:575: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      "Sure, here's the information from the context regarding the macronutrients and\n",
      "their roles in the human body:  **Macronutrients**  * Carbohydrates: Provide\n",
      "energy for various bodily functions. * Lipids: Assist in the formation of cell\n",
      "structure and provide stored energy. * Proteins: Are essential for tissue\n",
      "formation, cell repair, and hormone and enzyme production.  **Water**  * Water\n",
      "is crucial for maintaining blood volume, regulating body temperature, and\n",
      "facilitating chemical reactions in the body.  * Over 60% of total body weight is\n",
      "water, making it of utmost importance for bodily functions.\n",
      "Context items:\n",
      "Query: How do vitamins and minerals differ in their roles and importance for health?\n",
      "Answer:\n",
      "\n",
      "The context does not provide relevant passages about the differences between\n",
      "vitamins and minerals, so I cannot answer this question from the provided\n",
      "context.\n",
      "Context items:\n",
      "Query: Describe the process of digestion and absorption of nutrients in the human body.\n",
      "Answer:\n",
      "\n",
      "Sure, here's a summary of the process of digestion and absorption of nutrients\n",
      "in the human body:  **Step 1: Food intake and the mouth**  The process of\n",
      "digestion begins even before you put food into your mouth. When you feel hungry,\n",
      "your body sends a message to your brain that it is time to eat. Sights and\n",
      "smells influence your body’s preparedness for food. Smelling food sends a\n",
      "message to your brain. Your brain then tells the mouth to get ready, and you\n",
      "start to salivate in preparation for a meal.  **Step 2: Digestion**  The\n",
      "digestive system functions on two levels, mechanically to move and mix ingested\n",
      "food and chemically to break down large molecules. The digestive system is one\n",
      "of the eleven organ systems of the human body, and it is composed of several\n",
      "hollow tube-shaped organs including the mouth, pharynx, esophagus, stomach,\n",
      "small intestine, large intestine (colon), rectum, and anus.  **Step 3: Digestion\n",
      "and absorption of carbohydrates**  The process of digestion converts the large\n",
      "polymers in food to monomers that can be absorbed. Starches are broken down to\n",
      "monosaccharides, lipids are broken down to fatty acids, and proteins are broken\n",
      "down to amino acids. These monomers are absorbed into the bloodstream either\n",
      "directly, as is the case with monosaccharides and amino acids, or repackaged in\n",
      "intestinal cells for transport by an indirect route through lymphatic vessels,\n",
      "as is the case with most fatty acids and other fat-soluble molecules.  **In\n",
      "conclusion**, the process of digestion and absorption of nutrients in the human\n",
      "body is a complex and intricate process that ensures that the body has the\n",
      "nutrients it needs to function properly.\n",
      "Context items:\n",
      "Query: What role does fibre play in digestion? Name five fibre containing foods.\n",
      "Answer:\n",
      "\n",
      "The passage does not provide any information about the role of fiber in\n",
      "digestion, so I cannot answer this query from the context.\n",
      "Context items:\n",
      "Query: Explain the concept of energy balance and its importance in weight management.\n",
      "Answer:\n",
      "\n",
      "Sure, here is the answer to the user's query:  **Concept of Energy Balance**\n",
      "Energy balance refers to the overall balance between the intake and expenditure\n",
      "of nutrients, particularly carbohydrates, proteins, and fats. It is a crucial\n",
      "factor in maintaining body weight and overall health. When energy intake is\n",
      "equal to expenditure, the body maintains its weight. However, when energy intake\n",
      "is greater than expenditure, the body stores excess energy as fat. Conversely,\n",
      "when energy expenditure is higher than intake, the body uses its stores of fat\n",
      "and muscle tissue to provide energy.  **Importance of Energy Balance for Weight\n",
      "Management**  Maintaining energy balance is essential for weight management\n",
      "because it ensures that the body receives the necessary nutrients to perform\n",
      "various physiological functions, including metabolism, growth, and reproduction.\n",
      "When energy intake is balanced, the body can better regulate its weight,\n",
      "maintain muscle and bone health, and respond to physical demands.  The context\n",
      "mentions that weight management involves balancing energy input with energy\n",
      "expenditure. This means that the body's ability to burn calories should match\n",
      "the rate at which it takes in calories. When energy intake is greater than\n",
      "expenditure, the body stores excess calories as fat, contributing to weight\n",
      "gain. Conversely, when energy expenditure is higher than intake, the body\n",
      "utilizes stored energy sources to sustain life processes.\n",
      "Context items:\n",
      "Query: How often should infants be breastfed?\n",
      "Answer:\n",
      "\n",
      "The passage does not specify how often infants should be breastfed, only that it\n",
      "should be done to provide optimal nutrition.\n",
      "Context items:\n",
      "Query: What are symptoms of pellagra?\n",
      "Answer:\n",
      "\n",
      "The passage states that niacin deficiency, also known as pellagra, can cause\n",
      "symptoms such as diarrhea, dermatitis, dementia, and even death.\n",
      "Context items:\n",
      "Query: How does saliva help with digestion?\n",
      "Answer:\n",
      "\n",
      "The passage does not provide information about saliva's role in digestion, so I\n",
      "cannot answer this question from the provided context.\n",
      "Context items:\n",
      "Query: What is the RDI for protein per day?\n",
      "Answer:\n",
      "\n",
      "The context does not explicitly specify the RDI for protein per day, so I cannot\n",
      "answer this question from the provided context.\n",
      "Context items:\n",
      "Query: water soluble vitamins\n",
      "Answer:\n",
      "\n",
      "Sure, here are the relevant passages from the context:  - Water-soluble vitamins\n",
      "act in the cytosol of cells or in extracellular fluids such as blood; fat-\n",
      "soluble vitamins are largely responsible for protecting cell membranes from free\n",
      "radical damage.  - Vitamins are required for blood renewal and function.  - At\n",
      "insufficient levels in the diet these vitamins and minerals impair the health of\n",
      "blood and consequently the delivery of nutrients in and wastes out, amongst its\n",
      "many other functions.\n",
      "Context items:\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "query = random.choice(query_list)\n",
    "for query in query_list:\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    # Answer query with context and return context \n",
    "    answer, context_items = ask(query,\n",
    "                                embedding_model, \n",
    "                                collection,\n",
    "                                temperature=0.7,\n",
    "                                max_new_tokens=512,\n",
    "                                return_answer_only=False)\n",
    "\n",
    "    print(f\"Answer:\\n\")\n",
    "    print_wrapped(answer)\n",
    "    print(f\"Context items:\")\n",
    "    context_items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
